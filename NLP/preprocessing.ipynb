{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c19d1c7",
   "metadata": {},
   "source": [
    "### NLP\n",
    "\n",
    "- Basic Terminologies\n",
    "1. Corpus => Paragraph\n",
    "2. document => sentence\n",
    "3. word => word\n",
    "4. Vocabulary => Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bd88c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Apple Inc. is planning to open a new office in Bangalore, India by next year. \n",
    "Tim Cook, the CEO of Apple, announced this during a press conference on September 5, 2025. \n",
    "The company aims to hire more than 5,000 engineers specializing in AI and machine learning. \n",
    "Meanwhile, Google and Microsoft are also expanding their operations in India.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7addb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc17032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"My name is Prathamesh Gokulkar. I am learning Natural Language Processing using Python's NLTK library. NLTK is a powerful tool for working with human language data (text). It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65746056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My name is Prathamesh Gokulkar. I am learning Natural Language Processing using Python's NLTK library. NLTK is a powerful tool for working with human language data (text). It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c5e36",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The process of converting a sequence of text into smaller parts, known as tokens.  \n",
    "\n",
    "These tokens can be as small as characters or as long as words. The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bf2760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a6e0068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Prathamesh Gokulkar.',\n",
       " \"I am learning Natural Language Processing using Python's NLTK library.\",\n",
       " 'NLTK is a powerful tool for working with human language data (text).',\n",
       " 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "documents = sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b39f0c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Prathamesh',\n",
       " 'Gokulkar',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'using',\n",
       " 'Python',\n",
       " \"'s\",\n",
       " 'NLTK',\n",
       " 'library',\n",
       " '.',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'tool',\n",
       " 'for',\n",
       " 'working',\n",
       " 'with',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '(',\n",
       " 'text',\n",
       " ')',\n",
       " '.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'easy-to-use',\n",
       " 'interfaces',\n",
       " 'to',\n",
       " 'over',\n",
       " '50',\n",
       " 'corpora',\n",
       " 'and',\n",
       " 'lexical',\n",
       " 'resources',\n",
       " 'such',\n",
       " 'as',\n",
       " 'WordNet',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'a',\n",
       " 'suite',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'libraries',\n",
       " 'for',\n",
       " 'classification',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'and',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f26111f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Prathamesh',\n",
       " 'Gokulkar',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'using',\n",
       " 'Python',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " '.',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'tool',\n",
       " 'for',\n",
       " 'working',\n",
       " 'with',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '(',\n",
       " 'text',\n",
       " ').',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'easy',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'use',\n",
       " 'interfaces',\n",
       " 'to',\n",
       " 'over',\n",
       " '50',\n",
       " 'corpora',\n",
       " 'and',\n",
       " 'lexical',\n",
       " 'resources',\n",
       " 'such',\n",
       " 'as',\n",
       " 'WordNet',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'a',\n",
       " 'suite',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'libraries',\n",
       " 'for',\n",
       " 'classification',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'and',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# WordPunct Tokenization\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d2e0c",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is an important text-processing technique that reduces words to their base or root form by removing prefixes and suffixes.  \n",
    "This process standardizes words which helps to improve the efficiency and effectiveness of various natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2a0d0",
   "metadata": {},
   "source": [
    "### PorterStemmer\n",
    "The stemmed output is not guaranteed to be a meaningful word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47cf249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"running\", \"runner\", \"runs\", \"easily\", \"fairly\", \"fishing\", \"fished\", \"happiness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7aa63605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runner --> runner\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n",
      "fishing --> fish\n",
      "fished --> fish\n",
      "happiness --> happi\n"
     ]
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} --> {stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deafc7e",
   "metadata": {},
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57c70bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fe43f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runner --> runner\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n",
      "fishing --> fish\n",
      "fished --> fish\n",
      "happiness --> happi\n"
     ]
    }
   ],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} --> {stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715418dc",
   "metadata": {},
   "source": [
    "### Snowball Stemmer\n",
    "Enhanced version of the Porter Stemmer  \n",
    "One of the key advantages of this is that it supports multiple languages, making it a multilingual stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e588cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runner --> runner\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n",
      "fishing --> fish\n",
      "fished --> fish\n",
      "happiness --> happi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} --> {snow_stemmer.stem(word)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25814794",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Unlike stemming which simply removes prefixes or suffixes, it considers the word's meaning and part of speech (POS) and ensures that the base form is a valid word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c83591",
   "metadata": {},
   "source": [
    "### WordNet Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5671e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "938e7753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runner --> runner\n",
      "runs --> run\n",
      "easily --> easily\n",
      "fairly --> fairly\n",
      "fishing --> fish\n",
      "fished --> fish\n",
      "happiness --> happiness\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} --> {lemmatizer.lemmatize(word, pos= 'v') }\")\n",
    "    # pos = 'v' for verb\n",
    "    # pos = 'n' for noun\n",
    "    # pos = 'a' for adjective\n",
    "    # pos = 'r' for adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b3c2f",
   "metadata": {},
   "source": [
    "## Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e976d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword = stopwords.words('english')\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1752343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45b9b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'Prathamesh', 'Gokulkar', '.', 'I', 'learning', 'Natural', 'Language', 'Processing', 'using', 'Python', \"'s\", 'NLTK', 'library', '.', 'NLTK', 'powerful', 'tool', 'working', 'human', 'language', 'data', '(', 'text', ')', '.', 'It', 'provides', 'easy-to-use', 'interface', '50', 'corpus', 'lexical', 'resource', 'WordNet', ',', 'along', 'suite', 'text', 'processing', 'library', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [lemmatizer.lemmatize(word) for word in tokenized_corpus if word not in stopword]\n",
    "\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19733887",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08f798c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LOQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LOQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\LOQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf9ec90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "text = word_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb841d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = pos_tag(text)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c0ea61",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a1bc1a",
   "metadata": {},
   "source": [
    "NER require POS tagged text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c85f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\LOQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\LOQ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "#Necessary dowmloads for NER\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bb7a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Apple/NNP)\n",
      "  (ORGANIZATION Inc./NNP)\n",
      "  is/VBZ\n",
      "  planning/VBG\n",
      "  to/TO\n",
      "  open/VB\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  office/NN\n",
      "  in/IN\n",
      "  (GPE Bangalore/NNP)\n",
      "  ,/,\n",
      "  (GPE India/NNP)\n",
      "  by/IN\n",
      "  next/JJ\n",
      "  year/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (PERSON Tim/NNP)\n",
      "  (GPE Cook/NNP)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (ORGANIZATION CEO/NNP)\n",
      "  of/IN\n",
      "  (GPE Apple/NNP)\n",
      "  ,/,\n",
      "  announced/VBD\n",
      "  this/DT\n",
      "  during/IN\n",
      "  a/DT\n",
      "  press/NN\n",
      "  conference/NN\n",
      "  on/IN\n",
      "  September/NNP\n",
      "  5/CD\n",
      "  ,/,\n",
      "  2025/CD\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  company/NN\n",
      "  aims/VBZ\n",
      "  to/TO\n",
      "  hire/VB\n",
      "  more/JJR\n",
      "  than/IN\n",
      "  5,000/CD\n",
      "  engineers/NNS\n",
      "  specializing/VBG\n",
      "  in/IN\n",
      "  AI/NNP\n",
      "  and/CC\n",
      "  machine/NN\n",
      "  learning/NN\n",
      "  ./.)\n",
      "(S\n",
      "  Meanwhile/RB\n",
      "  ,/,\n",
      "  (PERSON Google/NNP)\n",
      "  and/CC\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  are/VBP\n",
      "  also/RB\n",
      "  expanding/VBG\n",
      "  their/PRP$\n",
      "  operations/NNS\n",
      "  in/IN\n",
      "  (GPE India/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(sample_text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "    print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb7c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19335188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
