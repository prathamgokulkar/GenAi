{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c19d1c7",
   "metadata": {},
   "source": [
    "### NLP\n",
    "\n",
    "- Basic Terminologies\n",
    "1. Corpus => Paragraph\n",
    "2. document => sentence\n",
    "3. word => word\n",
    "4. Vocabulary => Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7addb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc17032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"My name is Prathamesh Gokulkar. I am learning Natural Language Processing using Python's NLTK library. NLTK is a powerful tool for working with human language data (text). It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65746056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My name is Prathamesh Gokulkar. I am learning Natural Language Processing using Python's NLTK library. NLTK is a powerful tool for working with human language data (text). It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c5e36",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The process of converting a sequence of text into smaller parts, known as tokens.  \n",
    "\n",
    "These tokens can be as small as characters or as long as words. The primary reason this process matters is that it helps machines understand human language by breaking it down into bite-sized pieces, which are easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf2760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a6e0068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Prathamesh Gokulkar.',\n",
       " \"I am learning Natural Language Processing using Python's NLTK library.\",\n",
       " 'NLTK is a powerful tool for working with human language data (text).',\n",
       " 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "documents = sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39f0c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Prathamesh',\n",
       " 'Gokulkar',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'using',\n",
       " 'Python',\n",
       " \"'s\",\n",
       " 'NLTK',\n",
       " 'library',\n",
       " '.',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'tool',\n",
       " 'for',\n",
       " 'working',\n",
       " 'with',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '(',\n",
       " 'text',\n",
       " ')',\n",
       " '.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'easy-to-use',\n",
       " 'interfaces',\n",
       " 'to',\n",
       " 'over',\n",
       " '50',\n",
       " 'corpora',\n",
       " 'and',\n",
       " 'lexical',\n",
       " 'resources',\n",
       " 'such',\n",
       " 'as',\n",
       " 'WordNet',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'a',\n",
       " 'suite',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'libraries',\n",
       " 'for',\n",
       " 'classification',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'and',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f26111f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Prathamesh',\n",
       " 'Gokulkar',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'using',\n",
       " 'Python',\n",
       " \"'\",\n",
       " 's',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " '.',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'tool',\n",
       " 'for',\n",
       " 'working',\n",
       " 'with',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '(',\n",
       " 'text',\n",
       " ').',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'easy',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'use',\n",
       " 'interfaces',\n",
       " 'to',\n",
       " 'over',\n",
       " '50',\n",
       " 'corpora',\n",
       " 'and',\n",
       " 'lexical',\n",
       " 'resources',\n",
       " 'such',\n",
       " 'as',\n",
       " 'WordNet',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'a',\n",
       " 'suite',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'libraries',\n",
       " 'for',\n",
       " 'classification',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'and',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# WordPunct Tokenization\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d2e0c",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is an important text-processing technique that reduces words to their base or root form by removing prefixes and suffixes.  \n",
    "This process standardizes words which helps to improve the efficiency and effectiveness of various natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2a0d0",
   "metadata": {},
   "source": [
    "### PorterStemmer\n",
    "The stemmed output is not guaranteed to be a meaningful word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47cf249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"running\", \"runner\", \"runs\", \"easily\", \"fairly\", \"fishing\", \"fished\", \"happiness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa63605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runner --> runner\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n",
      "fishing --> fish\n",
      "fished --> fish\n",
      "happiness --> happi\n"
     ]
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} --> {stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deafc7e",
   "metadata": {},
   "source": [
    "### RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c70bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fe43f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runner --> runner\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n",
      "fishing --> fish\n",
      "fished --> fish\n",
      "happiness --> happi\n"
     ]
    }
   ],
   "source": [
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} --> {stemming.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715418dc",
   "metadata": {},
   "source": [
    "### Snowball Stemmer\n",
    "Enhanced version of the Porter Stemmer  \n",
    "One of the key advantages of this is that it supports multiple languages, making it a multilingual stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e588cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runner --> runner\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n",
      "fishing --> fish\n",
      "fished --> fish\n",
      "happiness --> happi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} --> {snow_stemmer.stem(word)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25814794",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Unlike stemming which simply removes prefixes or suffixes, it considers the word's meaning and part of speech (POS) and ensures that the base form is a valid word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c83591",
   "metadata": {},
   "source": [
    "### WordNet Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5671e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "938e7753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runner --> runner\n",
      "runs --> run\n",
      "easily --> easily\n",
      "fairly --> fairly\n",
      "fishing --> fish\n",
      "fished --> fish\n",
      "happiness --> happiness\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} --> {lemmatizer.lemmatize(word, pos= 'v') }\")\n",
    "    # pos = 'v' for verb\n",
    "    # pos = 'n' for noun\n",
    "    # pos = 'a' for adjective\n",
    "    # pos = 'r' for adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19733887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
